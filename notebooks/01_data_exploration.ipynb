{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e566b66e",
   "metadata": {},
   "source": [
    "# ä¸­æ–‡é…’åº—è¯„è®ºæƒ…æ„Ÿåˆ†æ - æ•°æ®æ¢ç´¢\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å°†å¯¹ChnSentiCorpé…’åº—è¯„è®ºæ•°æ®é›†è¿›è¡Œå…¨é¢çš„æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7190ee4",
   "metadata": {},
   "source": [
    "## 1. æ•°æ®åŠ è½½ä¸åŸºæœ¬ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c674d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "df = pd.read_csv('../ChnSentiCorp_htl_all.csv', encoding='utf-8')\n",
    "\n",
    "print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"\\næ•°æ®é›†åŸºæœ¬ä¿¡æ¯:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\næ•°æ®é›†å‰5è¡Œ:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "print(\"ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\næ•°æ®ç±»å‹:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\næ•°æ®é›†æè¿°æ€§ç»Ÿè®¡:\")\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14622db",
   "metadata": {},
   "source": [
    "## 2. æ ‡ç­¾åˆ†å¸ƒåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ‡ç­¾åˆ†å¸ƒ\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"æ ‡ç­¾åˆ†å¸ƒ:\")\n",
    "print(label_counts)\n",
    "print(f\"\\næ ‡ç­¾æ¯”ä¾‹:\")\n",
    "print(df['label'].value_counts(normalize=True))\n",
    "\n",
    "# å¯è§†åŒ–æ ‡ç­¾åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# é¥¼å›¾\n",
    "labels = ['æ­£é¢', 'è´Ÿé¢']\n",
    "axes[0].pie(label_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('æƒ…æ„Ÿæ ‡ç­¾åˆ†å¸ƒ - é¥¼å›¾')\n",
    "\n",
    "# æŸ±çŠ¶å›¾\n",
    "sns.countplot(data=df, x='label', ax=axes[1])\n",
    "axes[1].set_title('æƒ…æ„Ÿæ ‡ç­¾åˆ†å¸ƒ - æŸ±çŠ¶å›¾')\n",
    "axes[1].set_xlabel('æ ‡ç­¾ (0: è´Ÿé¢, 1: æ­£é¢)')\n",
    "axes[1].set_ylabel('æ•°é‡')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[1].text(i, v + 50, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f32cd",
   "metadata": {},
   "source": [
    "## 3. æ–‡æœ¬é•¿åº¦åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—æ–‡æœ¬é•¿åº¦\n",
    "df['text_length'] = df['review'].astype(str).apply(len)\n",
    "df['word_count'] = df['review'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# æ–‡æœ¬é•¿åº¦ç»Ÿè®¡\n",
    "print(\"æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# å¯è§†åŒ–æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# æ•´ä½“æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ\n",
    "axes[0].hist(df['text_length'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')\n",
    "axes[0].set_xlabel('å­—ç¬¦æ•°')\n",
    "axes[0].set_ylabel('é¢‘æ¬¡')\n",
    "\n",
    "# æŒ‰æ ‡ç­¾åˆ†ç»„çš„æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ\n",
    "for label in [0, 1]:\n",
    "    subset = df[df['label'] == label]['text_length']\n",
    "    axes[1].hist(subset, bins=30, alpha=0.6, \n",
    "                   label=f'æ ‡ç­¾ {label} ({\"è´Ÿé¢\" if label == 0 else \"æ­£é¢\"})')\n",
    "axes[1].set_title('æŒ‰æƒ…æ„Ÿæ ‡ç­¾åˆ†ç»„çš„æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')\n",
    "axes[1].set_xlabel('å­—ç¬¦æ•°')\n",
    "axes[1].set_ylabel('é¢‘æ¬¡')\n",
    "axes[1].legend()\n",
    "\n",
    "# æ–‡æœ¬é•¿åº¦ç®±çº¿å›¾\n",
    "sns.boxplot(data=df, x='label', y='text_length', ax=axes[2])\n",
    "axes[2].set_title('æŒ‰æ ‡ç­¾åˆ†ç»„çš„æ–‡æœ¬é•¿åº¦ç®±çº¿å›¾')\n",
    "axes[2].set_xlabel('æ ‡ç­¾ (0: è´Ÿé¢, 1: æ­£é¢)')\n",
    "axes[2].set_ylabel('å­—ç¬¦æ•°')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»Ÿè®¡ä¸åŒé•¿åº¦åŒºé—´çš„æ–‡æœ¬æ•°é‡\n",
    "length_bins = [0, 50, 100, 200, 500, 1000, float('inf')]\n",
    "length_labels = ['0-50', '51-100', '101-200', '201-500', '501-1000', '1000+']\n",
    "\n",
    "df['length_category'] = pd.cut(df['text_length'], bins=length_bins, labels=length_labels)\n",
    "\n",
    "print(\"ä¸åŒé•¿åº¦åŒºé—´çš„æ–‡æœ¬åˆ†å¸ƒ:\")\n",
    "length_dist = df['length_category'].value_counts().sort_index()\n",
    "print(length_dist)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "length_dist.plot(kind='bar')\n",
    "plt.title('ä¸åŒé•¿åº¦åŒºé—´çš„æ–‡æœ¬æ•°é‡åˆ†å¸ƒ')\n",
    "plt.xlabel('æ–‡æœ¬é•¿åº¦åŒºé—´')\n",
    "plt.ylabel('æ•°é‡')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, v in enumerate(length_dist.values):\n",
    "    plt.text(i, v + 20, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f88a9",
   "metadata": {},
   "source": [
    "## 4. æ–‡æœ¬å†…å®¹åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšæœºå±•ç¤ºä¸€äº›è¯„è®ºæ ·æœ¬\n",
    "print(\"=== æ­£é¢è¯„è®ºæ ·æœ¬ ===\")\n",
    "positive_samples = df[df['label'] == 1]['review'].sample(5)\n",
    "for i, review in enumerate(positive_samples, 1):\n",
    "    print(f\"{i}. {review}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n=== è´Ÿé¢è¯„è®ºæ ·æœ¬ ===\")\n",
    "negative_samples = df[df['label'] == 0]['review'].sample(5)\n",
    "for i, review in enumerate(negative_samples, 1):\n",
    "    print(f\"{i}. {review}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da429fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†è¯å’Œè¯é¢‘ç»Ÿè®¡\n",
    "def segment_text(text):\n",
    "    \"\"\"ä¸­æ–‡åˆ†è¯\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # æ¸…ç†æ–‡æœ¬\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9]', ' ', str(text))\n",
    "    # åˆ†è¯\n",
    "    words = jieba.cut(text)\n",
    "    # è¿‡æ»¤çŸ­è¯å’Œåœç”¨è¯\n",
    "    words = [word.strip() for word in words if len(word.strip()) > 1]\n",
    "    return words\n",
    "\n",
    "# å¯¹æ‰€æœ‰æ–‡æœ¬è¿›è¡Œåˆ†è¯\n",
    "print(\"æ­£åœ¨è¿›è¡Œä¸­æ–‡åˆ†è¯...\")\n",
    "df['words'] = df['review'].apply(segment_text)\n",
    "\n",
    "# ç»Ÿè®¡æ€»è¯é¢‘\n",
    "all_words = []\n",
    "for words in df['words']:\n",
    "    all_words.extend(words)\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"æ€»è¯æ±‡æ•°: {len(word_freq)}\")\n",
    "print(f\"æ€»è¯æ¬¡æ•°: {sum(word_freq.values())}\")\n",
    "\n",
    "print(\"\\næœ€å¸¸è§çš„20ä¸ªè¯:\")\n",
    "for word, freq in word_freq.most_common(20):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab30146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‰æƒ…æ„Ÿæ ‡ç­¾åˆ†åˆ«ç»Ÿè®¡è¯é¢‘\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "for idx, words in enumerate(df['words']):\n",
    "    if df.iloc[idx]['label'] == 1:\n",
    "        positive_words.extend(words)\n",
    "    else:\n",
    "        negative_words.extend(words)\n",
    "\n",
    "positive_freq = Counter(positive_words)\n",
    "negative_freq = Counter(negative_words)\n",
    "\n",
    "print(\"æ­£é¢è¯„è®ºä¸­æœ€å¸¸è§çš„15ä¸ªè¯:\")\n",
    "for word, freq in positive_freq.most_common(15):\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "print(\"\\nè´Ÿé¢è¯„è®ºä¸­æœ€å¸¸è§çš„15ä¸ªè¯:\")\n",
    "for word, freq in negative_freq.most_common(15):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56901df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è¯é¢‘åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# æ­£é¢è¯é¢‘\n",
    "pos_words, pos_freqs = zip(*positive_freq.most_common(15))\n",
    "axes[0].barh(range(len(pos_words)), pos_freqs)\n",
    "axes[0].set_yticks(range(len(pos_words)))\n",
    "axes[0].set_yticklabels(pos_words)\n",
    "axes[0].set_title('æ­£é¢è¯„è®ºé«˜é¢‘è¯')\n",
    "axes[0].set_xlabel('é¢‘æ¬¡')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# è´Ÿé¢è¯é¢‘\n",
    "neg_words, neg_freqs = zip(*negative_freq.most_common(15))\n",
    "axes[1].barh(range(len(neg_words)), neg_freqs, color='red', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(neg_words)))\n",
    "axes[1].set_yticklabels(neg_words)\n",
    "axes[1].set_title('è´Ÿé¢è¯„è®ºé«˜é¢‘è¯')\n",
    "axes[1].set_xlabel('é¢‘æ¬¡')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df070783",
   "metadata": {},
   "source": [
    "## 5. è¯äº‘å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆè¯äº‘\n",
    "# æ³¨æ„ï¼šéœ€è¦å…ˆå®‰è£…wordcloudåº“\n",
    "# pip install wordcloud\n",
    "\n",
    "import os\n",
    "import platform\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # è®¾ç½®ä¸­æ–‡å­—ä½“è·¯å¾„\n",
    "    def get_chinese_font():\n",
    "        \"\"\"è·å–ç³»ç»Ÿä¸­æ–‡å­—ä½“è·¯å¾„\"\"\"\n",
    "        system = platform.system()\n",
    "        if system == \"Windows\":\n",
    "            font_paths = [\n",
    "                \"C:/Windows/Fonts/simhei.ttf\",  # é»‘ä½“\n",
    "                \"C:/Windows/Fonts/msyh.ttc\",    # å¾®è½¯é›…é»‘\n",
    "                \"C:/Windows/Fonts/simsun.ttc\",  # å®‹ä½“\n",
    "            ]\n",
    "        elif system == \"Darwin\":  # macOS\n",
    "            font_paths = [\n",
    "                \"/System/Library/Fonts/PingFang.ttc\",\n",
    "                \"/System/Library/Fonts/STHeiti Light.ttc\",\n",
    "            ]\n",
    "        else:  # Linux\n",
    "            font_paths = [\n",
    "                \"/usr/share/fonts/truetype/wqy/wqy-microhei.ttc\",\n",
    "                \"/usr/share/fonts/truetype/arphic/ukai.ttc\",\n",
    "            ]\n",
    "        \n",
    "        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå­˜åœ¨çš„å­—ä½“æ–‡ä»¶\n",
    "        for font_path in font_paths:\n",
    "            if os.path.exists(font_path):\n",
    "                return font_path\n",
    "        return None\n",
    "    \n",
    "    chinese_font = get_chinese_font()\n",
    "    print(f\"ä½¿ç”¨å­—ä½“: {chinese_font}\")\n",
    "    \n",
    "    # æ­£é¢è¯„è®ºè¯äº‘\n",
    "    positive_text = ' '.join(positive_words)\n",
    "    wordcloud_pos = WordCloud(\n",
    "        font_path=chinese_font,\n",
    "        width=800, height=400,\n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='Blues',\n",
    "        collocations=False,  # é¿å…é‡å¤è¯ç»„\n",
    "        relative_scaling=0.5,\n",
    "        random_state=42\n",
    "    ).generate(positive_text)\n",
    "    \n",
    "    # è´Ÿé¢è¯„è®ºè¯äº‘\n",
    "    negative_text = ' '.join(negative_words)\n",
    "    wordcloud_neg = WordCloud(\n",
    "        font_path=chinese_font,\n",
    "        width=800, height=400,\n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='Reds',\n",
    "        collocations=False,  # é¿å…é‡å¤è¯ç»„\n",
    "        relative_scaling=0.5,\n",
    "        random_state=42\n",
    "    ).generate(negative_text)\n",
    "    \n",
    "    # æ˜¾ç¤ºè¯äº‘\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    axes[0].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title('æ­£é¢è¯„è®ºè¯äº‘', fontsize=16)\n",
    "    \n",
    "    axes[1].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('è´Ÿé¢è¯„è®ºè¯äº‘', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ä¿å­˜è¯äº‘å›¾ç‰‡\n",
    "    try:\n",
    "        os.makedirs('../results', exist_ok=True)\n",
    "        fig.savefig('../results/wordcloud_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"è¯äº‘å›¾ç‰‡å·²ä¿å­˜åˆ° ../results/wordcloud_analysis.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {e}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"è¯·å®‰è£…wordcloudåº“ï¼špip install wordcloud\")\n",
    "except Exception as e:\n",
    "    print(f\"ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™: {e}\")\n",
    "    print(\"å¦‚æœé‡åˆ°å­—ä½“é—®é¢˜ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿä¸­å®‰è£…äº†ä¸­æ–‡å­—ä½“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8a0e7",
   "metadata": {},
   "source": [
    "## 6. ç‰¹æ®Šå­—ç¬¦å’Œæ¨¡å¼åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æç‰¹æ®Šå­—ç¬¦å’Œæ¨¡å¼\n",
    "def analyze_text_patterns(text):\n",
    "    \"\"\"åˆ†ææ–‡æœ¬æ¨¡å¼\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {\n",
    "            'has_emoji': False,\n",
    "            'has_punctuation': False,\n",
    "            'has_english': False,\n",
    "            'has_numbers': False,\n",
    "            'exclamation_count': 0,\n",
    "            'question_count': 0\n",
    "        }\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    return {\n",
    "        'has_emoji': bool(re.search(r'[ğŸ˜€-ğŸ™]', text)),\n",
    "        'has_punctuation': bool(re.search(r'[ï¼ï¼Ÿã€‚ï¼Œï¼›ï¼š]', text)),\n",
    "        'has_english': bool(re.search(r'[a-zA-Z]', text)),\n",
    "        'has_numbers': bool(re.search(r'\\d', text)),\n",
    "        'exclamation_count': text.count('ï¼') + text.count('!'),\n",
    "        'question_count': text.count('ï¼Ÿ') + text.count('?')\n",
    "    }\n",
    "\n",
    "# åº”ç”¨æ¨¡å¼åˆ†æ\n",
    "pattern_analysis = df['review'].apply(analyze_text_patterns)\n",
    "pattern_df = pd.DataFrame(pattern_analysis.tolist())\n",
    "\n",
    "# åˆå¹¶åˆ°åŸæ•°æ®æ¡†\n",
    "df = pd.concat([df, pattern_df], axis=1)\n",
    "\n",
    "# ç»Ÿè®¡å„ç§æ¨¡å¼çš„å‡ºç°é¢‘ç‡\n",
    "print(\"æ–‡æœ¬æ¨¡å¼ç»Ÿè®¡:\")\n",
    "print(f\"åŒ…å«è¡¨æƒ…ç¬¦å·: {df['has_emoji'].sum()} ({df['has_emoji'].mean():.2%})\")\n",
    "print(f\"åŒ…å«æ ‡ç‚¹ç¬¦å·: {df['has_punctuation'].sum()} ({df['has_punctuation'].mean():.2%})\")\n",
    "print(f\"åŒ…å«è‹±æ–‡å­—ç¬¦: {df['has_english'].sum()} ({df['has_english'].mean():.2%})\")\n",
    "print(f\"åŒ…å«æ•°å­—: {df['has_numbers'].sum()} ({df['has_numbers'].mean():.2%})\")\n",
    "print(f\"å¹³å‡æ„Ÿå¹å·æ•°é‡: {df['exclamation_count'].mean():.2f}\")\n",
    "print(f\"å¹³å‡é—®å·æ•°é‡: {df['question_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfe67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‰æƒ…æ„Ÿæ ‡ç­¾åˆ†ææ–‡æœ¬æ¨¡å¼\n",
    "print(\"æŒ‰æƒ…æ„Ÿæ ‡ç­¾åˆ†ç»„çš„æ–‡æœ¬æ¨¡å¼åˆ†æ:\")\n",
    "for label in [0, 1]:\n",
    "    subset = df[df['label'] == label]\n",
    "    label_name = \"è´Ÿé¢\" if label == 0 else \"æ­£é¢\"\n",
    "    print(f\"\\n{label_name}è¯„è®º:\")\n",
    "    print(f\"  åŒ…å«æ ‡ç‚¹ç¬¦å·: {subset['has_punctuation'].mean():.2%}\")\n",
    "    print(f\"  åŒ…å«è‹±æ–‡å­—ç¬¦: {subset['has_english'].mean():.2%}\")\n",
    "    print(f\"  åŒ…å«æ•°å­—: {subset['has_numbers'].mean():.2%}\")\n",
    "    print(f\"  å¹³å‡æ„Ÿå¹å·æ•°é‡: {subset['exclamation_count'].mean():.2f}\")\n",
    "    print(f\"  å¹³å‡é—®å·æ•°é‡: {subset['question_count'].mean():.2f}\")\n",
    "    print(f\"  å¹³å‡æ–‡æœ¬é•¿åº¦: {subset['text_length'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f18850",
   "metadata": {},
   "source": [
    "## 7. æ•°æ®è´¨é‡è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18731f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥æ•°æ®è´¨é‡é—®é¢˜\n",
    "print(\"æ•°æ®è´¨é‡è¯„ä¼°:\")\n",
    "\n",
    "# 1. é‡å¤è¯„è®º\n",
    "duplicate_reviews = df['review'].duplicated().sum()\n",
    "print(f\"é‡å¤è¯„è®ºæ•°é‡: {duplicate_reviews}\")\n",
    "\n",
    "# 2. è¿‡çŸ­è¯„è®º\n",
    "very_short = (df['text_length'] < 10).sum()\n",
    "print(f\"è¿‡çŸ­è¯„è®ºï¼ˆ<10å­—ç¬¦ï¼‰æ•°é‡: {very_short}\")\n",
    "\n",
    "# 3. è¿‡é•¿è¯„è®º\n",
    "very_long = (df['text_length'] > 1000).sum()\n",
    "print(f\"è¿‡é•¿è¯„è®ºï¼ˆ>1000å­—ç¬¦ï¼‰æ•°é‡: {very_long}\")\n",
    "\n",
    "# 4. ç©ºç™½æˆ–æ— æ„ä¹‰è¯„è®º\n",
    "empty_or_meaningless = df['review'].astype(str).str.strip().str.len() == 0\n",
    "print(f\"ç©ºç™½è¯„è®ºæ•°é‡: {empty_or_meaningless.sum()}\")\n",
    "\n",
    "# å±•ç¤ºä¸€äº›é—®é¢˜æ•°æ®æ ·æœ¬\n",
    "if duplicate_reviews > 0:\n",
    "    print(\"\\né‡å¤è¯„è®ºç¤ºä¾‹:\")\n",
    "    duplicates = df[df['review'].duplicated(keep=False)]['review'].unique()[:3]\n",
    "    for i, dup in enumerate(duplicates, 1):\n",
    "        print(f\"{i}. {dup}\")\n",
    "\n",
    "if very_short > 0:\n",
    "    print(\"\\nè¿‡çŸ­è¯„è®ºç¤ºä¾‹:\")\n",
    "    short_reviews = df[df['text_length'] < 10]['review'].head(3)\n",
    "    for i, review in enumerate(short_reviews, 1):\n",
    "        print(f\"{i}. {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24392a27",
   "metadata": {},
   "source": [
    "## 8. æ€»ç»“å’Œå»ºè®®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fabbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é›†æ€»ç»“\n",
    "print(\"=== æ•°æ®é›†æ€»ç»“ ===\")\n",
    "print(f\"æ•°æ®é›†å¤§å°: {len(df):,} æ¡è¯„è®º\")\n",
    "print(f\"æ­£é¢è¯„è®º: {(df['label'] == 1).sum():,} æ¡ ({(df['label'] == 1).mean():.1%})\")\n",
    "print(f\"è´Ÿé¢è¯„è®º: {(df['label'] == 0).sum():,} æ¡ ({(df['label'] == 0).mean():.1%})\")\n",
    "print(f\"å¹³å‡æ–‡æœ¬é•¿åº¦: {df['text_length'].mean():.1f} å­—ç¬¦\")\n",
    "print(f\"æ–‡æœ¬é•¿åº¦ä¸­ä½æ•°: {df['text_length'].median():.1f} å­—ç¬¦\")\n",
    "print(f\"è¯æ±‡æ€»æ•°: {len(word_freq):,} ä¸ªä¸åŒè¯æ±‡\")\n",
    "\n",
    "print(\"\\n=== æ•°æ®é¢„å¤„ç†å»ºè®® ===\")\n",
    "print(\"1. æ•°æ®åŸºæœ¬å¹³è¡¡ï¼Œæ— éœ€ç‰¹æ®Šé‡‡æ ·å¤„ç†\")\n",
    "print(\"2. æ–‡æœ¬é•¿åº¦å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®è®¾ç½®åˆé€‚çš„æœ€å¤§é•¿åº¦æˆªæ–­\")\n",
    "print(\"3. åŒ…å«æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—ï¼Œéœ€è¦è€ƒè™‘æ˜¯å¦ä¿ç•™\")\n",
    "print(\"4. å»ºè®®ç§»é™¤é‡å¤è¯„è®ºå’Œè¿‡çŸ­è¯„è®º\")\n",
    "print(\"5. å¯ä»¥è€ƒè™‘ä½¿ç”¨åˆ†è¯ç»“æœè¾…åŠ©BERTæ¨¡å‹\")\n",
    "\n",
    "print(\"\\n=== å»ºè®®çš„æ•°æ®é¢„å¤„ç†å‚æ•° ===\")\n",
    "print(f\"æ¨èæœ€å¤§åºåˆ—é•¿åº¦: {int(df['text_length'].quantile(0.95))} (è¦†ç›–95%æ•°æ®)\")\n",
    "print(f\"æœ€å°æ–‡æœ¬é•¿åº¦é˜ˆå€¼: 10 å­—ç¬¦\")\n",
    "print(f\"æ‰¹å¤„ç†å¤§å°å»ºè®®: 16-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ce6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜åˆ†æç»“æœ\n",
    "analysis_summary = {\n",
    "    'dataset_size': len(df),\n",
    "    'positive_count': int((df['label'] == 1).sum()),\n",
    "    'negative_count': int((df['label'] == 0).sum()),\n",
    "    'avg_text_length': float(df['text_length'].mean()),\n",
    "    'median_text_length': float(df['text_length'].median()),\n",
    "    'vocabulary_size': len(word_freq),\n",
    "    'duplicate_count': int(duplicate_reviews),\n",
    "    'short_text_count': int(very_short),\n",
    "    'long_text_count': int(very_long),\n",
    "    'recommended_max_length': int(df['text_length'].quantile(0.95)),\n",
    "    'top_positive_words': positive_freq.most_common(20),\n",
    "    'top_negative_words': negative_freq.most_common(20)\n",
    "}\n",
    "\n",
    "# ä¿å­˜åˆ°æ–‡ä»¶\n",
    "import json\n",
    "with open('../data/analysis/eda_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"åˆ†æç»“æœå·²ä¿å­˜åˆ° ../data/analysis/eda_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
