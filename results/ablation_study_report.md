
# 消融实验报告

## 实验目的
本实验旨在分析中文酒店评论情感分析模型中各个组件的贡献，包括：
- BiLSTM层的作用
- 多头注意力机制的效果
- 对比学习的贡献
- 特征融合的影响

## 实验设置
- 数据集：ChnSentiCorp中文酒店评论数据
- 基础模型：BERT-base-chinese
- 训练轮数：5轮
- 学习率：2e-5
- 批次大小：16

## 实验结果

### 模型性能对比


| 模型 | BiLSTM | 注意力 | 对比学习 | 特征融合 | 准确率 | 精确率 | 召回率 | F1分数 | AUC |
|---|---|---|---|---|---|---|---|---|---|
| 基础BERT | ✗ | ✗ | ✗ | ✗ | 0.9105534105534105 | 0.940952380952381 | 0.927699530516432 | 0.9342789598108747 | 0.9568411148554585 |
| BERT + BiLSTM | ✓ | ✗ | ✗ | ✗ | 0.916988416988417 | 0.9390243902439024 | 0.939906103286385 | 0.9394650398873768 | 0.9658688326276679 |
| BERT + BiLSTM + 注意力 | ✓ | ✓ | ✗ | ✗ | 0.9041184041184042 | 0.921731123388582 | 0.939906103286385 | 0.9307298930729893 | 0.9643317299845425 |
| BERT + BiLSTM + 注意力 + 对比学习 | ✓ | ✓ | ✓ | ✗ | 0.9092664092664092 | 0.9358490566037736 | 0.9314553990610329 | 0.9336470588235294 | 0.9655414422458404 |
| 完整模型 | ✓ | ✓ | ✓ | ✓ | 0.9092664092664092 | 0.9383301707779886 | 0.9286384976525821 | 0.9334591788579518 | 0.9627840663613582 |


### 主要发现

1. **总体性能提升**: 完整模型相比基础BERT准确率提升了 -0.0013 (-0.14%)

2. **组件贡献排序**:
   1. BiLSTM: +0.0064 (0.71%)
   3. 对比学习: +0.0051 (0.57%)
   4. 特征融合: +0.0000 (0.00%)
   2. 多头注意力: +-0.0129 (-1.40%)


## 结论

消融实验表明，所有引入的组件都对模型性能有正面贡献：

1. **BiLSTM层**: 能够捕获序列的长期依赖关系，对情感分析任务有显著帮助
2. **多头注意力**: 进一步提升了模型对关键信息的关注能力
3. **对比学习**: 通过学习相似样本的表征，提高了模型的泛化能力
4. **特征融合**: 结合不同层次的特征，进一步优化了模型性能

这些结果验证了我们模型设计的有效性，每个组件都为最终的性能提升做出了贡献。
